{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecte des données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Chargement des fichiers CSV \n",
    "traffic_data = pd.read_csv('trafic_reseau.csv')\n",
    "timestamp_data = pd.read_csv('horodatage du trafic.csv')\n",
    "\n",
    "# Affichage des premières lignes de chaque fichier pour voir leur structure\n",
    "traffic_data.head(), timestamp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul de l'entropie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Définition de la fonction pour calculer l'entropie de Shannon\n",
    "def calculate_entropy(data):\n",
    "    value_counts = data.value_counts()\n",
    "    probabilities = value_counts / value_counts.sum()\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "# Calcul de l'entropie de Shannon pour chaque seconde dans la colonne 'Destination'\n",
    "entropy_per_second = traffic_data.groupby('Second')['Destination'].apply(calculate_entropy)\n",
    "\n",
    "# Stockage des résultats dans un DataFrame\n",
    "entropy_df = entropy_per_second.reset_index()\n",
    "entropy_df.columns = ['Second', 'Entropy']\n",
    "\n",
    "entropy_df.head()\n",
    "\n",
    "\n",
    "# Création d'un graphe pour visualiser l'entropie par seconde\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(entropy_df['Second'], entropy_df['Entropy'], marker='o', linestyle='-')\n",
    "plt.title('Entropie de Shannon par Seconde')\n",
    "plt.xlabel('Second')\n",
    "plt.ylabel('Entropie')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul des FTT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Chargement des données\n",
    "traffic_data = pd.read_csv('trafic_reseau.csv')\n",
    "\n",
    "# Fonction pour calculer la FFT des longueurs des paquets\n",
    "def calculate_fft(data):\n",
    "    fft_values = np.abs(fft(data))\n",
    "    return fft_values\n",
    "\n",
    "# Fonction pour détecter les pics dans la FFT\n",
    "def find_fft_peaks(data, height_ratio=0.5):\n",
    "    # Définition du seuil à un pourcentage de l'amplitude du pic le plus élevé\n",
    "    height_threshold = np.max(data) * height_ratio\n",
    "    peaks, properties = find_peaks(data, height=height_threshold)\n",
    "    return peaks, properties['peak_heights']\n",
    "\n",
    "# Groupement des données par seconde et calcul de la FFT\n",
    "grouped_data = traffic_data.groupby('Second')['Length']\n",
    "fft_results = grouped_data.apply(calculate_fft)\n",
    "\n",
    "# Création d'une liste pour enregistrer le nombre de pics pour chaque seconde\n",
    "peak_counts = []\n",
    "\n",
    "# Affichage des graphiques pour chaque seconde avec indication des pics\n",
    "fig, axes = plt.subplots(nrows=min(5, len(fft_results)), ncols=1, figsize=(10, 20), sharex=True)\n",
    "for (second, fft_data), ax in zip(fft_results.items(), axes.flatten()):\n",
    "    peaks, peak_heights = find_fft_peaks(fft_data)\n",
    "    peak_counts.append((second, len(peaks)))  # Enregistrement du nombre de pics\n",
    "    ax.plot(fft_data, marker='o', linestyle='-')\n",
    "    ax.plot(peaks, fft_data[peaks], \"x\")\n",
    "    ax.set_title(f'Magnitude de la FFT pour la seconde {second} - {len(peaks)} pics')\n",
    "    ax.set_xlabel('Fréquence (bins)')\n",
    "    ax.set_ylabel('Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage de la liste des secondes avec le nombre de pics\n",
    "print(peak_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion des caractéristiques : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# Chargement des données\n",
    "traffic_data = pd.read_csv('trafic_reseau.csv')\n",
    "\n",
    "# Calcul de l'entropie de Shannon\n",
    "def calculate_entropy(data):\n",
    "    value_counts = data.value_counts()\n",
    "    probabilities = value_counts / value_counts.sum()\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "# Calcul de la FFT et détection des pics\n",
    "def calculate_fft(data):\n",
    "    fft_values = np.abs(fft(data))\n",
    "    return fft_values\n",
    "\n",
    "def find_fft_peaks(data, height_ratio=0.5):\n",
    "    height_threshold = np.max(data) * height_ratio\n",
    "    peaks, properties = find_peaks(data, height=height_threshold)\n",
    "    return len(peaks)\n",
    "\n",
    "# Application des calculs\n",
    "entropy_results = traffic_data.groupby('Second')['Destination'].apply(calculate_entropy)\n",
    "fft_results = traffic_data.groupby('Second')['Length'].apply(calculate_fft)\n",
    "peak_counts = traffic_data.groupby('Second')['Length'].apply(lambda x: find_fft_peaks(calculate_fft(x)))\n",
    "\n",
    "# Création d'un DataFrame combiné\n",
    "combined_results = pd.DataFrame({\n",
    "    'Second': entropy_results.index,\n",
    "    'Entropy': entropy_results.values,\n",
    "    'Peak Count': peak_counts.values\n",
    "})\n",
    "\n",
    "# Affichage des résultats\n",
    "print(combined_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Supposons que 'combined_results' est le DataFrame contenant les secondes, l'entropie, et le compte de pics\n",
    "# Calcul des valeurs Min et Max pour l'entropie et le compte de pics\n",
    "timestamp_data = pd.read_csv('horodatage du trafic.csv')  # Assurez-vous que le nom du fichier est correct\n",
    "\n",
    "min_entropy = combined_results['Entropy'].min()\n",
    "max_entropy = combined_results['Entropy'].max()\n",
    "min_peaks = combined_results['Peak Count'].min()\n",
    "max_peaks = combined_results['Peak Count'].max()\n",
    "\n",
    "# Application de la normalisation Min-Max\n",
    "combined_results['Normalized Entropy'] = (combined_results['Entropy'] - min_entropy) / (max_entropy - min_entropy)\n",
    "combined_results['Normalized Peak Count'] = (combined_results['Peak Count'] )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_results.drop(columns=['Entropy', 'Peak Count'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Chargement des données de trafic réseau et de l'horodatage\n",
    "traffic_data = pd.read_csv('trafic_reseau.csv')\n",
    "timestamp_data = pd.read_csv('horodatage du trafic.csv')\n",
    "\n",
    "# Normalisation et préparation des caractéristiques comme décrit précédemment\n",
    "# Supposons que combined_results contient déjà les caractéristiques normalisées\n",
    "# Ajout de la colonne 'Attack' depuis timestamp_data\n",
    "combined_results['Attack'] = timestamp_data['Attack']\n",
    "\n",
    "# Remplacement des valeurs '-' par '0' pour indiquer l'absence d'attaque et '1' pour une attaque\n",
    "combined_results['Attack'] = combined_results['Attack'].replace({'-': 0, '1': 1})\n",
    "\n",
    "# Affichage des données prêtes pour l'entraînement avec la colonne 'Attack' incluse\n",
    "print(combined_results.head())\n",
    "\n",
    "# À ce stade, vous pouvez diviser les données en ensembles d'entraînement et de test\n",
    "# Exemple simple de division en utilisant une méthode basique\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = combined_results[['Normalized Entropy', 'Normalized Peak Count']]\n",
    "labels = combined_results['Attack']\n",
    "# Sauvegarder le DataFrame 'combined_results' dans un fichier CSV\n",
    "combined_results.to_csv('combined_results.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Charger les données à partir du fichier CSV\n",
    "combined_results = pd.read_csv('combined_results.csv')\n",
    "\n",
    "# Séparer les caractéristiques et les étiquettes\n",
    "features = combined_results[['Normalized Entropy', 'Normalized Peak Count']]\n",
    "labels = combined_results['Attack']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.20, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialiser le classificateur XGBoost\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Entraîner le classificateur XGBoost\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour les données de test\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Calcul de la matrice de confusion pour XGBoost\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "# Affichage de la matrice de confusion sous forme de heatmap pour XGBoost\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix (XGBoost)')\n",
    "plt.show()\n",
    "\n",
    "# Évaluation des performances du modèle XGBoost avec d'autres métriques\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prédire les probabilités pour les données de test pour le calcul de la courbe ROC\n",
    "y_pred_proba_xgb = xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calcul des métriques de performance\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)  # Correction: ajout de cette ligne pour calculer l'exactitude\n",
    "precision = precision_score(y_test, y_pred_xgb, average='binary')  # Assurez-vous de définir l'argument 'average' correctement\n",
    "recall = recall_score(y_test, y_pred_xgb, average='binary')\n",
    "f1 = f1_score(y_test, y_pred_xgb, average='binary')\n",
    "\n",
    "# Calcul de la courbe ROC et de l'aire sous la courbe (AUC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)  # Utilisation de probabilités plutôt que des étiquettes\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Affichage des métriques\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# Affichage de la courbe ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
